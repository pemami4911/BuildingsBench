{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9962cd65-d3fe-46ea-b146-953f596e2a97",
   "metadata": {},
   "source": [
    "# Script for splitting iid train/val sets from the tuning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12337f31-0b8a-4cad-a9bd-0df63df5dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from buildings_bench.data.buildings900K import Buildings900K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5998fb0c-e87d-4952-a16a-c2f2a220a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = Path(\"/projects/foundation/eulp/v1.1.0/BuildingsBench/metadata_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee050e-4771-4b5e-8ef5-8d52ddacf157",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_weather_features = ['temperature', 'humidity', 'wind_speed', 'wind_direction', 'global_horizontal_radiation', \n",
    "                              'direct_normal_radiation', 'diffuse_horizontal_radiation']\n",
    "dataset_path = Path(os.environ.get('BUILDINGS_BENCH', ''))\n",
    "dataset = Buildings900K(dataset_path,\n",
    "                       index_file=metadata_path / \"comcap_350k_tune.idx\",\n",
    "                       context_len=0,\n",
    "                       pred_len=-1,\n",
    "                       weather=g_weather_features,\n",
    "                       use_com_buildings_chars=True,\n",
    "                       use_text_embedding=False,\n",
    "                       building_description=False,\n",
    "                       surrogate_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1a75a2-5551-4a59-90d6-50ea85ca53fa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "train_set (998900, 1)\n",
      "train_set (998900, 1)\n",
      "train_set (998900, 1)\n",
      "train_set (998900, 1)\n",
      "train_set (998900, 19)\n",
      "train_set (998900, 1)\n",
      "train_set (998900, 1)\n",
      "train_set (998900, 1)\n",
      "train_set (998900, 1)\n",
      "train_set (998900, 1)\n",
      "train_set (998900, 1)\n",
      "train_set (998900, 1)\n",
      "train_set (998900, 1)\n",
      "train_set (998900, 1)\n",
      "val_set (99890, 1)\n",
      "val_set (99890, 1)\n",
      "val_set (99890, 1)\n",
      "val_set (99890, 1)\n",
      "val_set (99890, 19)\n",
      "val_set (99890, 1)\n",
      "val_set (99890, 1)\n",
      "val_set (99890, 1)\n",
      "val_set (99890, 1)\n",
      "val_set (99890, 1)\n",
      "val_set (99890, 1)\n",
      "val_set (99890, 1)\n",
      "val_set (99890, 1)\n",
      "val_set (99890, 1)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    \"train_set\": defaultdict(list),\n",
    "    \"val_set\"  : defaultdict(list),\n",
    "}\n",
    "\n",
    "features = [\"day_of_year\", \"day_of_week\", \"hour_of_day\", \\\n",
    "                    \"load\", \"building_char\", \"building_id\", \"dataset_id\"] + g_weather_features\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "    building_data = dataset[i]\n",
    "    \n",
    "    random_idx = np.random.choice(8734, 110, replace=False) # 8734 total possible hours, 100 hours train, 10 hours val\n",
    "    train_idx, val_idx = random_idx[:100], random_idx[100:]\n",
    "    \n",
    "    # for train_set\n",
    "    for feature in features:\n",
    "        if feature in [\"building_id\", \"dataset_id\"]:\n",
    "            data[\"train_set\"][feature].append(np.repeat(np.array([[building_data[feature]]]), len(train_idx), axis=0))\n",
    "        else:\n",
    "            data[\"train_set\"][feature].append(building_data[feature][train_idx])\n",
    "            \n",
    "    # for val_set\n",
    "    for feature in features:\n",
    "        if feature in [\"building_id\", \"dataset_id\"]:\n",
    "            data[\"val_set\"][feature].append(np.repeat(np.array([[building_data[feature]]]), len(val_idx), axis=0))\n",
    "        else:\n",
    "            data[\"val_set\"][feature].append(building_data[feature][val_idx])\n",
    "            \n",
    "for split_name in [\"train_set\", \"val_set\"]:\n",
    "    for feature in features:\n",
    "        data[split_name][feature] = np.vstack(data[split_name][feature])\n",
    "\n",
    "    for feature in features:\n",
    "        print(split_name, data[split_name][feature].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02e46106-6c2c-43be-8de3-f5396fdc6434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(998900, 29)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_feature = [\"day_of_year\", \"day_of_week\", \"hour_of_day\"] + g_weather_features + [\"building_char\"]\n",
    "X_train = np.hstack([data[\"train_set\"][f] for f in X_feature])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e635e015-30bb-48be-a891-da59b2e7bfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99890, 29)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = np.hstack([data[\"val_set\"][f] for f in X_feature])\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762b8257-ab96-4370-a1b9-b25918c46001",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(metadata_path / \"comcap_tune_X_train.npz\", data=X_train)\n",
    "np.savez(metadata_path / \"comcap_tune_X_val.npz\", data=X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b11698-e43b-4843-9205-21f14ea3a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = data[\"train_set\"][\"load\"]\n",
    "Y_val = data[\"val_set\"][\"load\"]\n",
    "np.savez(metadata_path / \"comcap_tune_Y_train.npz\", data=Y_train)\n",
    "np.savez(metadata_path / \"comcap_tune_Y_val.npz\", data=Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a53364-fd0a-4bf2-b5d4-5b8ba0d8400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train = np.hstack([data[\"train_set\"][f] for f in [\"building_id\", \"dataset_id\"]])\n",
    "meta_val = np.hstack([data[\"val_set\"][f] for f in [\"building_id\", \"dataset_id\"]])\n",
    "\n",
    "np.savez(metadata_path / \"comcap_tune_meta_train.npz\", data=meta_train)\n",
    "np.savez(metadata_path / \"comcap_tune_meta_val.npz\", data=meta_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f96d96c6-8482-4d54-a479-a033c4a12066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99890, 29) (99890, 1) (99890, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# use only 10% of training data\n",
    "X_train_small, _, Y_train_small, _, meta_train_small, _ = train_test_split(X_train, Y_train, meta_train, test_size=0.9, shuffle=True, random_state=0)\n",
    "print(X_train_small.shape, Y_train_small.shape, meta_train_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e42d64-3e16-4e5d-a699-56be807649f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(metadata_path / \"comcap_tune_small_X_train.npz\", data=X_train_small)\n",
    "np.savez(metadata_path / \"comcap_tune_small_Y_train.npz\", data=Y_train_small)\n",
    "np.savez(metadata_path / \"comcap_tune_small_meta_train.npz\", data=meta_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8423f15c-5671-4d47-b521-1fbe6d4475f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# X = np.load(metadata_path / \"comcap_X.npz\")[\"X\"]\n",
    "# Y = np.load(metadata_path / \"comcap_Y.npz\")[\"Y\"]\n",
    "# meta = np.load(metadata_path / \"comcap_meta.npz\")[\"meta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d48b63cc-f227-4933-a642-2d87edbf63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shaphypetune import BoostSearch, BoostRFE, BoostRFA, BoostBoruta\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from lightgbm import *\n",
    "\n",
    "# X_train, X_test, y_train, y_test, meta_train, meta_test = train_test_split(\n",
    "#     X, Y, meta, test_size=50000, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16ea155b-e781-4c57-82a3-d2611acc3244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (299425, 32)\n",
      "X_test (50000, 32)\n",
      "y_train (299425, 1)\n",
      "y_test (50000, 1)\n",
      "meta_train (299425, 2)\n",
      "meta_test (50000, 2)\n"
     ]
    }
   ],
   "source": [
    "# print(\"X_train\", X_train.shape)\n",
    "# print(\"X_test\", X_test.shape)\n",
    "# print(\"y_train\", y_train.shape)\n",
    "# print(\"y_test\", y_test.shape)\n",
    "# print(\"meta_train\", meta_train.shape)\n",
    "# print(\"meta_test\", meta_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "153c9afa-c62c-44a5-99db-ec653761736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test (40000, 32)\n",
      "X_val (10000, 32)\n",
      "y_test (40000, 1)\n",
      "y_val (10000, 1)\n",
      "meta_test (40000, 2)\n",
      "meta_val (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "# X_test, X_val, y_test, y_val, meta_test, meta_val = train_test_split(\n",
    "#     X_test, y_test, meta_test, test_size=40000, shuffle=True, random_state=0)\n",
    "# print(\"X_test\", X_test.shape)\n",
    "# print(\"X_val\", X_val.shape)\n",
    "# print(\"y_test\", y_test.shape)\n",
    "# print(\"y_val\", y_val.shape)\n",
    "# print(\"meta_test\", meta_test.shape)\n",
    "# print(\"meta_val\", meta_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b73950cc-7a42-4124-b887-d9cb940cc7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(metadata_path / \"comcap_X_train.npz\", data=X_train)\n",
    "# np.savez(metadata_path / \"comcap_X_test.npz\", data=X_test)\n",
    "# np.savez(metadata_path / \"comcap_X_val.npz\", data=X_val)\n",
    "# np.savez(metadata_path / \"comcap_Y_train.npz\", data=y_train)\n",
    "# np.savez(metadata_path / \"comcap_Y_test.npz\", data=y_test)\n",
    "# np.savez(metadata_path / \"comcap_Y_val.npz\", data=y_val)\n",
    "# np.savez(metadata_path / \"comcap_meta_train.npz\", data=meta_train)\n",
    "# np.savez(metadata_path / \"comcap_meta_test.npz\", data=meta_test)\n",
    "# np.savez(metadata_path / \"comcap_meta_val.npz\", data=meta_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e68f49-c40e-4730-b088-53a1b9c7ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# X_train = np.load(metadata_path / \"comcap_tune_X_train.npz\")[\"data\"]\n",
    "# Y_train = np.load(metadata_path / \"comcap_tune_Y_train.npz\")[\"data\"]\n",
    "# X_val = np.load(metadata_path / \"comcap_tune_X_val.npz\")[\"data\"]\n",
    "# Y_val = np.load(metadata_path / \"comcap_tune_Y_val.npz\")[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d9a3b9-1b43-47a1-9ccc-ee5e40f81ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.vstack([X_train, X_val])\n",
    "# Y = np.vstack([Y_train, Y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4bfeccc-64b6-41f9-b3ad-1f62e95dae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_small, _, Y_small, _ = train_test_split(X, Y, test_size=0.99, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eff68bf8-b6dd-4435-9c65-0cdd64416dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_small_train, X_small_val, Y_small_train, Y_small_val = train_test_split(X_small, Y_small, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e35609b4-1e16-4744-89aa-c02b7ac38460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(metadata_path / \"comcap_tune_X_small_train.npz\", data=X_small_train)\n",
    "# np.savez(metadata_path / \"comcap_tune_Y_small_train.npz\", data=Y_small_train)\n",
    "# np.savez(metadata_path / \"comcap_tune_X_small_val.npz\", data=X_small_val)\n",
    "# np.savez(metadata_path / \"comcap_tune_Y_small_val.npz\", data=Y_small_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3af57c-1aec-471d-8aff-2dc561ac14d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
